{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e499334c-1c35-4a31-bf3d-ecff4db6b40d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## initial pyspark session\n",
    "\n",
    "# import relevant libraries and columns\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import array, size, udf,col, explode, countDistinct, col, size, avg, col, log, lit, coalesce, expr, array, col, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b573c12b-2ba6-4363-9696-2e9170e61360",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# gets the static people and companies tables from linked in\n",
    "profiles = spark.read.parquet('/linkedin/people')\n",
    "companies = spark.read.parquet('/linkedin/companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2e5407-1163-4150-9cde-106752144ab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## filter the profiles who doesn't have or had any position, and have at least one education\n",
    "\n",
    "# filter profiles without position\n",
    "profiles = profiles.filter(col('position').isNotNull() & \n",
    "                    (col('position') != '--'))\n",
    "\n",
    "#filter profiles without past positions\n",
    "array_length_udf = udf(lambda x: len(x) if x is not None else 0, IntegerType())\n",
    "profiles = profiles.withColumn('len', array_length_udf(col('experience')))\n",
    "profiles = profiles.filter(col('len') > 1)\n",
    "\n",
    "#filter profiles without education\n",
    "profiles = profiles.withColumn('len2', array_length_udf(col('education')))\n",
    "profiles = profiles.filter(col('len2') > 0)\n",
    "\n",
    "print(profiles.count())\n",
    "profiles.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c1ff5aa-5bb4-454b-8c25-276a1ba142ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## pre processing for experience\n",
    "\n",
    "def experience_preprocessing(profiles_df):\n",
    "    jobs_table = profiles_df.select(['id', 'experience', 'current_company:name', 'position'])\n",
    "    #jobs_table.cache()\n",
    "    # jobs_table.display(10)\n",
    "\n",
    "\n",
    "    ## companies' names\n",
    "    def extract_companies(experience, current_company):\n",
    "        companies = [current_company.lower()] if current_company else []\n",
    "        for company_dict in experience:\n",
    "            company = company_dict['company']\n",
    "            if company:\n",
    "                company_lower = company.lower()\n",
    "                # if company_lower not in companies:\n",
    "                companies.append(company_lower)\n",
    "        return companies\n",
    "\n",
    "    # turn the data about companies to document for bm25\n",
    "    companiesUdf = udf(extract_companies, ArrayType(StringType()))\n",
    "    jobs_table = jobs_table.\\\n",
    "            withColumn('companies', companiesUdf(jobs_table['experience'], jobs_table['current_company:name']))\n",
    "\n",
    "    ## positions\n",
    "    def extract_positions(experience, current_position):\n",
    "        positions = []\n",
    "        if current_position:\n",
    "            current_position = current_position.lower()\n",
    "            if ' at ' in current_position:\n",
    "                current_position = current_position[0:current_position.index(' at ')]\n",
    "            if ' . ' in current_position:\n",
    "                current_position = current_position[0:current_position.index(' . ')]\n",
    "            positions.append(current_position)\n",
    "            \n",
    "        if experience:\n",
    "            for company_dict in experience:\n",
    "                if 'positions' in company_dict and company_dict['positions']:\n",
    "                    for position_dict in company_dict['positions']:\n",
    "                        position_title = position_dict['title']\n",
    "                        if position_title:\n",
    "                            position = position_title.lower()\n",
    "                            # if position not in positions:\n",
    "                            positions.append(position)\n",
    "        return positions\n",
    "\n",
    "    # turn the data about positions to document for bm25\n",
    "    positionsUdf = udf(extract_positions, ArrayType(StringType()))\n",
    "    jobs_table = jobs_table.\\\n",
    "            withColumn('positions', positionsUdf(jobs_table['experience'], jobs_table['position']))\n",
    "\n",
    "    # combine them\n",
    "    mergeCols = udf(lambda companies, positions: companies + positions, ArrayType(StringType()))\n",
    "    jobs_table = jobs_table.withColumn(\"full_experience\", mergeCols(col(\"companies\"), col(\"positions\")))\n",
    "\n",
    "    # jobs_table.display(10)\n",
    "    return jobs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63345140-2444-42e0-977c-aee75e9ea739",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## pre processing for education\n",
    "\n",
    "def education_preprocessing(profiles_df):\n",
    "    study_table = profiles_df.select(['id', 'education', 'educations_details', 'сourses'])\n",
    "    # study_table.display(10)\n",
    "\n",
    "    ## education column\n",
    "    def extract_degrees(educations):\n",
    "        degrees = []\n",
    "        for education_dict in educations:\n",
    "            degree = education_dict['degree']\n",
    "            if degree:\n",
    "                degree_lower = degree.lower()\n",
    "                # if degree_lower not in degrees:\n",
    "                degrees.append(degree_lower)\n",
    "        return degrees\n",
    "\n",
    "    # turn the data about degrees to document for bm25\n",
    "    degreesUdf = udf(extract_degrees, ArrayType(StringType()))\n",
    "    study_table = study_table.\\\n",
    "            withColumn('degrees', degreesUdf(study_table['education']))\n",
    "\n",
    "    def extract_fields(educations):\n",
    "        fields = []\n",
    "        for education_dict in educations:\n",
    "            field = education_dict['field']\n",
    "            if field:\n",
    "                field_lower = field.lower()\n",
    "                # if degree_lower not in degrees:\n",
    "                fields.append(field_lower)\n",
    "        return fields\n",
    "\n",
    "    # turn the data about fields to document for bm25\n",
    "    fieldsUdf = udf(extract_fields, ArrayType(StringType()))\n",
    "    study_table = study_table.\\\n",
    "            withColumn('fields', fieldsUdf(study_table['education']))\n",
    "\n",
    "    def extract_titles(educations):\n",
    "        titles = []\n",
    "        for education_dict in educations:\n",
    "            title = education_dict['title']\n",
    "            if title:\n",
    "                title_lower = title.lower()\n",
    "                # if degree_lower not in degrees:\n",
    "                titles.append(title_lower)\n",
    "        return titles\n",
    "\n",
    "    # turn the data about titles to document for bm25\n",
    "    titlesUdf = udf(extract_titles, ArrayType(StringType()))\n",
    "    study_table = study_table.\\\n",
    "            withColumn('titles', titlesUdf(study_table['education']))\n",
    "\n",
    "    ## details columns\n",
    "    titlesUdf = udf(lambda x: [x.lower()] if x else [], ArrayType(StringType()))\n",
    "    study_table = study_table.\\\n",
    "            withColumn('details', titlesUdf(study_table['educations_details']))\n",
    "\n",
    "    ## courses column\n",
    "    def extract_titles(courses):\n",
    "        titles = []\n",
    "        for course_dict in courses:\n",
    "            title = course_dict['title']\n",
    "            if title:\n",
    "                title_lower = title.lower()\n",
    "                # if degree_lower not in degrees:\n",
    "                titles.append(title_lower)\n",
    "        return titles\n",
    "\n",
    "    # turn the data about courses to document for bm25\n",
    "    titlesUdf = udf(extract_titles, ArrayType(StringType()))\n",
    "    study_table = study_table.\\\n",
    "            withColumn('courses_titles', titlesUdf(study_table['сourses']))\n",
    "\n",
    "    def extract_subtitles(courses):\n",
    "        subtitles = []\n",
    "        for course_dict in courses:\n",
    "            subtitle = course_dict['subtitle'].replace('-','')\n",
    "            if subtitle:\n",
    "                subtitle_lower = subtitle.lower()\n",
    "                # if degree_lower not in degrees:\n",
    "                subtitles.append(subtitle_lower)\n",
    "        return subtitles\n",
    "\n",
    "    # turn the data about subtitles to document for bm25\n",
    "    subtitlesUdf = udf(extract_subtitles, ArrayType(StringType()))\n",
    "    study_table = study_table.\\\n",
    "            withColumn('courses_subtitles', titlesUdf(study_table['сourses']))\n",
    "\n",
    "    # combine them\n",
    "    mergeCols = udf(lambda x: [item for sublist in x for item in sublist], ArrayType(StringType()))\n",
    "    study_table = study_table.withColumn(\"full_education\", mergeCols(array(col(\"degrees\"), col(\"fields\"), col(\"titles\"), col(\"details\"), col(\"courses_titles\"), col(\"courses_subtitles\"))))\n",
    "\n",
    "    #study_table.display(10)\n",
    "    return study_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e824d9f-8391-4836-b515-a99491a041a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## pre processing for general information\n",
    "\n",
    "def information_preprocessing(profiles_df):\n",
    "    info_table = profiles_df.select(['id', 'about', 'certifications'])\n",
    "    info_table.display(10)\n",
    "\n",
    "    ## about column\n",
    "    infoUdf = udf(lambda x: x.split(' ') if x else '', ArrayType(StringType()))\n",
    "    info_table = info_table.\\\n",
    "            withColumn('abouts', infoUdf(info_table['about']))\n",
    "\n",
    "    ## certifications column\n",
    "    def extract_metas(certifications):\n",
    "        metas = []\n",
    "        for certification_dict in certifications:\n",
    "            meta = certification_dict['meta']\n",
    "            if meta:\n",
    "                meta_lower = meta.lower()\n",
    "                # if degree_lower not in degrees:\n",
    "                metas.append(meta_lower)\n",
    "        return metas\n",
    "\n",
    "    # turn the data about metas to document for bm25\n",
    "    metasUdf = udf(extract_metas, ArrayType(StringType()))\n",
    "    info_table = info_table.\\\n",
    "            withColumn('metas', metasUdf(info_table['certifications']))\n",
    "\n",
    "    def extract_titles(certifications):\n",
    "        titles = []\n",
    "        for certification_dict in certifications:\n",
    "            title = certification_dict['title']\n",
    "            if title:\n",
    "                title_lower = title.lower()\n",
    "                # if degree_lower not in degrees:\n",
    "                titles.append(title_lower)\n",
    "        return titles\n",
    "\n",
    "    # turn the data about certifications titles to document for bm25\n",
    "    titlesUdf = udf(extract_titles, ArrayType(StringType()))\n",
    "    info_table = info_table.\\\n",
    "            withColumn('certifications_titles', titlesUdf(info_table['certifications']))\n",
    "\n",
    "    def extract_subtitles(certifications):\n",
    "        subtitles = []\n",
    "        for certification_dict in certifications:\n",
    "            subtitle = certification_dict['subtitle'].replace('-','')\n",
    "            if subtitle:\n",
    "                subtitle_lower = subtitle.lower()\n",
    "                # if degree_lower not in degrees:\n",
    "                subtitles.append(subtitle_lower)\n",
    "        return subtitles\n",
    "\n",
    "    # turn the data about certifications subtitles to document for bm25\n",
    "    subtitlesUdf = udf(extract_subtitles, ArrayType(StringType()))\n",
    "    info_table = info_table.\\\n",
    "            withColumn('certifications_subtitles', titlesUdf(info_table['certifications']))\n",
    "\n",
    "    # combine them\n",
    "    mergeCols = udf(lambda x: [item for sublist in x if sublist and all(sublist) for item in sublist], ArrayType(StringType()))\n",
    "    info_table = info_table.withColumn(\"information\", mergeCols(array(col(\"abouts\"), col(\"metas\"), col(\"certifications_titles\"), col(\"certifications_subtitles\"))))\n",
    "\n",
    "    #info_table.display(10)\n",
    "    return info_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "850d563c-2249-4e1c-b618-90d9af723b5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply the pre processing for all segmants\n",
    "\n",
    "# pre processing for education \n",
    "study_table = education_preprocessing(profiles)\n",
    "\n",
    "# pre processing\n",
    "jobs_table = experience_preprocessing(profiles)\n",
    "\n",
    "#pre processing for imformation\n",
    "info_table = information_preprocessing(profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ed933d-4b46-457c-ad84-d0d0a9eccd21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## preper given table to bm25 on certain column in it(create IR tf-idf data)\n",
    "\n",
    "def preper_bm25(jobs_table, column):\n",
    "    jobs_table_exploded = jobs_table.withColumn(\"token\", explode(f\"{column}\"))\n",
    "\n",
    "    # calculating df\n",
    "    jobs_table_df = jobs_table_exploded.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"doc_freq\"))\n",
    "    # jobs_table_df.display(10)\n",
    "\n",
    "    # calculating tf\n",
    "    jobs_table_tf = jobs_table_exploded.groupBy(\"id\", \"token\").count().withColumnRenamed('count', 'term_freq')\n",
    "    # jobs_table_tf.display(10)\n",
    "\n",
    "    # Calculate the length of each document\n",
    "    jobs_table_size = jobs_table.withColumn(\"doc_length\", size(f\"{column}\")).select('id','doc_length')\n",
    "    # jobs_table_size.display(10)\n",
    "\n",
    "    # calculate total number of documents and average document length\n",
    "    N = jobs_table.select(countDistinct(\"id\")).collect()[0][0]\n",
    "    avg_doc_len = jobs_table_size.groupBy().avg(\"doc_length\").collect()[0][0]\n",
    "    # print(\"Number of rows in the table:\", N)\n",
    "    # print(\"Average value of the column:\", avg_doc_len)\n",
    "\n",
    "    # Join all calculation\n",
    "    jobs_table_joined = jobs_table_tf.join(jobs_table_df, \"token\").join(jobs_table_size, 'id')\n",
    "    # jobs_table_joined.display(10)\n",
    "    \n",
    "    return jobs_table_joined, N, avg_doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0727da9-d74e-499b-8dbf-0f6c91cf1592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preper the segmants in tf-idf manner\n",
    "\n",
    "# prepare the experience to bm25\n",
    "jobs_table_joined, N, avg_doc_len = preper_bm25(jobs_table, 'full_experience')\n",
    "\n",
    "# prepare the education to bm25\n",
    "study_table_joined, N, avg_doc_len = preper_bm25(study_table, 'full_education')\n",
    "\n",
    "# prepare the information to bm25\n",
    "info_table_joined, N, avg_doc_len = preper_bm25(info_table, 'information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec7fd427-4d3c-4b70-8f6e-bf95fd825798",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ## interact with new profile not from profiles table\n",
    "\n",
    "# profile_table\n",
    "\n",
    "\n",
    "# # pre processing for education \n",
    "# new_study_table = education_preprocessing(profile_table)\n",
    "\n",
    "# # pre processing\n",
    "# new_jobs_table = experience_preprocessing(profile_table)\n",
    "\n",
    "# #pre processing for imformation\n",
    "# new_info_table = information_preprocessing(profile_table)\n",
    "\n",
    "\n",
    "# # get the experience query for the new profile\n",
    "# specific_profile_experience = new_jobs_table.first()['full_experience']\n",
    "\n",
    "# # get the education query for the new profile\n",
    "# specific_profile_education = new_study_table.first()['full_education']\n",
    "\n",
    "# # get the query for the new profile\n",
    "# specific_profile_info = new_info_table.first()['information']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d7e80c-3a13-433a-9e1c-fb922c9ea448",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## interact with new profile from profiles table\n",
    "\n",
    "# get the new profile id\n",
    "specific_profile_id = profiles.filter(col('id')=='denise-rathburn-9138a961').select('id').first()['id']\n",
    "print(specific_profile_id)\n",
    "\n",
    "# get the experience query for the new profile\n",
    "specific_profile_experience = jobs_table.filter(col('id')==specific_profile_id).first()['full_experience']\n",
    "\n",
    "# get the education query for the new profile\n",
    "specific_profile_education = study_table.filter(col('id')==specific_profile_id).first()['full_education']\n",
    "\n",
    "# get the query for the new profile\n",
    "specific_profile_info = info_table.filter(col('id')==specific_profile_id).first()['information']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "912d8684-7dd1-45ea-9f0b-8ce5df1c7ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## given table with tf-idf data, return the score of the bm25 for a specific profile\n",
    "\n",
    "def get_bm25(joined, N, avg_doc_len, specific_profile_values):\n",
    "\n",
    "    # Define constants k1 and b\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "\n",
    "    # go over the term in the profile query\n",
    "    for i, term in enumerate(specific_profile_values):\n",
    "        if i != specific_profile_values.index(term):\n",
    "            joined = joined.withColumn(f'bm25_score_{i}', col(f'bm25_score_{specific_profile_values.index(term)}'))\n",
    "        \n",
    "        # if its the first time\n",
    "        else:\n",
    "            # filter profile with those term\n",
    "            filtered_df = joined.filter(col(\"token\") == term)\n",
    "\n",
    "            # Calculate BM25 score for the specific term\n",
    "            filtered_df = filtered_df.withColumn(f\"bm25_score_{i}\", log((N - filtered_df[\"doc_freq\"] + 0.5) / \n",
    "                            (filtered_df[\"doc_freq\"] + 0.5)) * \\\n",
    "                        (filtered_df[\"term_freq\"] * (k1 + 1)) / (filtered_df[\"term_freq\"] + k1 * (1 - b + b * filtered_df[\"doc_length\"] / avg_doc_len)))\n",
    "            \n",
    "            # add to total bm25 score\n",
    "            joined = joined.join(filtered_df.select(\"id\", f\"bm25_score_{i}\"), \"id\", \"left_outer\")\n",
    "            joined = joined.fillna(0)\n",
    "\n",
    "    # write the total bm25 score\n",
    "    joined = joined.withColumn('bm25_score', expr('+'.join([f\"bm25_score_{i}\"\\\n",
    "                                    for i in range(len(specific_profile_values))])))\n",
    "\n",
    "    ranked_profiles = joined.select('id','bm25_score').distinct()\n",
    "    ranked_profiles = ranked_profiles.orderBy(ranked_profiles['bm25_score'].desc())\n",
    "    # ranked_profiles.display(10)\n",
    "\n",
    "    return ranked_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be11bb0-03f1-4395-b118-84dc5b26882d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## rank the profiles based on experience\n",
    "\n",
    "# calculate bm25 scores\n",
    "ranked_profiles_experience = get_bm25(jobs_table_joined, N, avg_doc_len, specific_profile_experience)\n",
    "\n",
    "# normelize the scores\n",
    "max_value = ranked_profiles_experience.selectExpr(\"max(bm25_score)\").collect()[0][0]\n",
    "ranked_profiles_experience = ranked_profiles_experience.withColumn(\"bm25_score\", col('bm25_score') / max_value)\n",
    "\n",
    "# display and save\n",
    "ranked_profiles_experience.display(10)\n",
    "ranked_profiles_experience.write.csv(\"/mnt/lab94290/results/experience_table.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe2721b-d2ac-418a-bff6-d715f48d1ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## rank the profiles based on education\n",
    "\n",
    "# calculate bm25 scores\n",
    "ranked_profiles_education = get_bm25(study_table_joined, N, avg_doc_len, specific_profile_education)\n",
    "\n",
    "# normelize the scores\n",
    "max_value = ranked_profiles_education.selectExpr(\"max(bm25_score)\").collect()[0][0]\n",
    "ranked_profiles_education = ranked_profiles_education.withColumn(\"bm25_score\", col('bm25_score') / max_value)\n",
    "\n",
    "# display and save\n",
    "ranked_profiles_education.display(10)\n",
    "ranked_profiles_education.write.csv(\"/mnt/lab94290/results/education_table.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5811fd-bcf4-4b3f-9eab-e27da843ef4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## rank the profiles based on general information\n",
    "\n",
    "# calculate bm25 scores\n",
    "ranked_profiles_info = get_bm25(info_table_joined, N, avg_doc_len, specific_profile_info)\n",
    "\n",
    "# normelize the scores\n",
    "max_value = ranked_profiles_info.selectExpr(\"max(bm25_score)\").collect()[0][0]\n",
    "ranked_profiles_info = ranked_profiles_info.withColumn(\"bm25_score\", col('bm25_score') / max_value)\n",
    "\n",
    "# display and save\n",
    "ranked_profiles_info.display(10)\n",
    "ranked_profiles_info.write.csv(\"/mnt/lab94290/results/information_table.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34775a09-31fe-4d16-88a1-372e4ef429b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# file_path = \"/FileStore/\"\n",
    "# experience = spark.read.csv(file_path+'experience_1.csv', header=True, inferSchema=True)\n",
    "# education = spark.read.csv(file_path+'education_1.csv', header=True, inferSchema=True)\n",
    "# information = spark.read.csv(file_path+'information_1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "experience = spark.read.csv(\"/mnt/lab94290/results/experience_table.csv\", header=True, inferSchema=True).limit(200)\n",
    "education = spark.read.csv(\"/mnt/lab94290/results/education_table.csv\", header=True, inferSchema=True).limit(200)\n",
    "information = spark.read.csv(\"/mnt/lab94290/results/information_table.csv\", header=True, inferSchema=True).limit(200)\n",
    "\n",
    "all_score = profiles.join(experience, \"id\", \"left_outer\").withColumnRenamed('bm25_score','experience_score')\n",
    "all_score = all_score.join(education, \"id\", \"left_outer\").withColumnRenamed('bm25_score','education_score')\n",
    "all_score = all_score.join(information, \"id\", \"left_outer\").withColumnRenamed('bm25_score','information_score')\n",
    "\n",
    "wanted_weights = [3,1,1]\n",
    "\n",
    "all_score = all_score.\\\n",
    "        withColumn(\"experience_score\", when(col(\"experience_score\").isNull(), 0).otherwise(wanted_weights[0]*col(\"experience_score\"))) \\\n",
    "       .withColumn(\"education_score\", when(col(\"education_score\").isNull(), 0).otherwise(wanted_weights[1]*col(\"education_score\"))) \\\n",
    "       .withColumn(\"information_score\", when(col(\"information_score\").isNull(), 0).otherwise(wanted_weights[2]*col(\"information_score\")))\n",
    "score_columns = ['experience_score', 'education_score', 'information_score']\n",
    "total_score = all_score.withColumn('total_score', expr('+'.join([f\"{column}\"\\\n",
    "                                     for column in score_columns])))\n",
    "\n",
    "ranked_profiles = total_score.orderBy(total_score['total_score'].desc())\n",
    "ranked_profiles.display(10)\n",
    "\n",
    "# ranked_profiles.select('id','total_score').head(100).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0415dd16-8f6c-4a28-91f4-879ef6072a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## used for visualization - unrunable\n",
    "\n",
    "# top20 = ranked_profiles.limit(20).select('id')\n",
    "# top20 = top20.join(jobs_table.select('id','full_experience'), \"id\", \"left_outer\")\n",
    "# top20 = top20.join(study_table.select('id','full_education'), \"id\", \"left_outer\")\n",
    "# top20 = top20.join(info_table.select('id','information'), \"id\", \"left_outer\")\n",
    "# mergeCols = udf(lambda x: [item for sublist in x for item in sublist], ArrayType(StringType()))\n",
    "# top20 = top20.withColumn('full_data', mergeCols(array(col('full_experience'),col('full_education'),col('information'))))\n",
    "# top20.display()\n",
    "\n",
    "# top20_exploded = top20.withColumn(\"word\", explode('full_data'))\n",
    "# top20_exploded.display()\n",
    "\n",
    "# from pyspark.sql.functions import collect_list\n",
    "# word_list = top20_exploded.select(collect_list(\"word\")).collect()[0][0]\n",
    "# word_string = \", \".join(word_list)\n",
    "\n",
    "# def removing(list, word):\n",
    "#     while word in list:\n",
    "#         list.remove(word)\n",
    "#     return list\n",
    "\n",
    "# text_list = removing(text_list, 'and')\n",
    "# text_list = removing(text_list, 'to')\n",
    "# text_list = removing(text_list, 'I')\n",
    "# text_list = removing(text_list, 'a')\n",
    "# text_list = removing(text_list, 'on')\n",
    "# text_list = removing(text_list, 'with')\n",
    "# text_list = removing(text_list, 'of')\n",
    "# text_list = removing(text_list, 'that')\n",
    "# text_list = removing(text_list, 'in')\n",
    "# text_list = removing(text_list, 'of')\n",
    "# text_list = removing(text_list, 'any')\n",
    "# text_list = removing(text_list, 'the')\n",
    "# text_list = removing(text_list, 'am')\n",
    "# text_list = removing(text_list, 'his')\n",
    "# text_list = removing(text_list, 'for')\n",
    "\n",
    "# text_list"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Collection Lab - Project - BM25 model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
